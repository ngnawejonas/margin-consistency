# -*- coding: utf-8 -*-
"""cifar10 autoattack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hARkBQpRwNh-iFNBJKLOqKNQEcKk8Qgm
"""

# !pip install git+https://github.com/fra31/auto-attack

# https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/
# Load in relevant libraries, and alias where appropriate
# from autoattack import AutoAttack 
import argparse
import os
import sys
import wandb
import yaml
import random
from tqdm import tqdm
from pprint import pprint
import numpy as np
import torch
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
from .pkg import losses
from .pkg.cor import compute_correlation
from .pkg.dataset import get_CIFAR10, get_MNIST
from .pkg.small_cnn import SmallCNN
from .pkg.resnet import ResNet18


# sys.path.append('../')
# from model import ResNetNormed
# sys.path.append('../pretrained')
# from resnet import resnet18

def parse_args(args: list) -> argparse.Namespace:
  """Parse command line parameters.

  :param args: command line parameters as list of strings (for example
      ``["--help"]``).
  :return: command line parameters namespace.
  """
  parser = argparse.ArgumentParser(
      description="Train the models for this experiment."
  )

  parser.add_argument(
      "--no-cuda", action="store_true", default=False, help="disables CUDA training"
  )
  parser.add_argument(
      "--dataset-path",
      default="/home-local2/jongn2.extra.nobkp/data",
      help="the path to the dataset",
      type=str,
  )
  parser.add_argument(
      "--cpus-per-trial",
      default=1,
      help="the number of CPU cores to use per trial",
      type=int,
  )
  parser.add_argument(
      "--project-name",
      help="the name of the Weights and Biases project to save the results",
      # required=True,
      default='',
      type=str,
  )
  parser.add_argument(
      "--dataset",
      help="dataset used",
      required=False,
      type=str,
  )
  parser.add_argument(
      "--loss",
      help="loss used",
      required=False,
      type=str,
  )
  parser.add_argument(
      "--paramfile",
      help="param file to use",
      default="",
      required=False,
      type=str,
    )

  return parser.parse_args(args)

def ddp_setup():
    init_process_group('nccl')

def set_seeds(seed):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)

def log(msg_dict, wandblog=False):
  if wandblog:
    wandb.log(msg_dict)
  else:
    pprint(msg_dict)

class LRScheduler:
    def __init__(self, optimizer, base_lr=0.01) -> None:
        self.optimizer = optimizer
        self.epoch = 1 #when placed at the end
        self.base_lr = base_lr
        self.new_lr = base_lr

    def step(self) -> None:
        """decrease the learning rate"""
        lr = self.base_lr
        if self.epoch >= 55:
            lr = self.base_lr * 0.1
        if self.epoch >= 75:
            lr = self.base_lr * 0.01
        if self.epoch >= 90:
            lr = self.base_lr * 0.001
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        self.epoch += 1
        self.new_lr = lr

    def get_last_lr(self):
        return self.new_lr
  
def configure_optimizers(model, params): 
    learning_rate = params[params['dataset']]['learning_rate']
    opt_params = params[params['dataset']]['opt_params']
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, **opt_params)
    scheduler = None
    if params['dataset'] == 'cifar10':    
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])
    if params['dataset'] == 'mnist':
        scheduler = LRScheduler(optimizer, base_lr=learning_rate)
    return optimizer, scheduler

def get_cm_objects(params, args, distributed):
    prefix = f"{args.project_name}_" if args.project_name else ""
    if params['dataset'] == 'mnist':
        ckptpath = f"./checkpoints/{prefix}{params['loss']}_ckpt.pt"
        train_dataset = get_MNIST(train=True)
        model = SmallCNN()
    else:
        ckptpath = f"./checkpoints/{prefix}{params['loss']}_r18_ckpt.pt"
        train_dataset = get_CIFAR10(train=True)
        model = ResNet18() #resnet18()
        # model = ResNetNormed(model)
    valid_size= 5000
    train_set, val_set = torch.utils.data.random_split(train_dataset,\
                                [len(train_dataset) - valid_size, valid_size])
    if distributed:
        train_loader = torch.utils.data.DataLoader(train_set, pin_memory=True, 
                                                shuffle=False,
                                                sampler=DistributedSampler(train_set),
                                                batch_size=params['batch_size'])
    else:
        train_loader = torch.utils.data.DataLoader(train_set, pin_memory=True, 
                                                    num_workers=2, shuffle=True,
                                                    batch_size=params['batch_size'])
    # val_loader = None
    val_loader = torch.utils.data.DataLoader(val_set, pin_memory=True, 
                                        num_workers=2, batch_size=params['batch_size'])
    cm_objects={'model': model, 
                'train_loader': train_loader,
                'val_loader': val_loader,
                'ckptpath': ckptpath}
    return cm_objects

def restore_model_if_exists(model, gpu_id, ckptpath):
    model = model.to(gpu_id)
    epochs_run = 0
    if os.path.exists(ckptpath):
        model, epochs_run = load_checkpoint(model, ckptpath)
    return model, epochs_run

def load_train_objects(params, args, distributed=True):
    train_objects = get_cm_objects(params, args, distributed)
    if params['loss'] is None:
        raise Exception('loss must be specified')
    else:
        loss_fn = getattr(losses, params['loss'])
        loss_args = params[params['dataset']]['loss_args']

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    gpu_id = int(os.environ["LOCAL_RANK"]) if distributed else device

    model, epochs_run = restore_model_if_exists(train_objects['model'],
                                                gpu_id, train_objects['ckptpath'])
    if distributed:   
        model = DDP(model, device_ids=[gpu_id], broadcast_buffers=False)
    #Setting the optimizer with the model parameters and learning rate
    optimizer, scheduler = configure_optimizers(model, params)
    train_objects.update({'model': model, 
                   'optimizer': optimizer, 
                   'scheduler': scheduler,
                   'gpu_id': gpu_id, 
                   'epochs_run': epochs_run,
                   'loss_fn': loss_fn,
                   'loss_args': loss_args,
                   'wandblog': params['wandblog'],
                   'save_every': params['save_every'],
                   'num_epochs': params['num_epochs'],
                   'distributed': distributed})
    return train_objects

def save_checkpoint(model, epoch, ckptpath, distributed=True):
    checkpoint= dict()
    checkpoint["EPOCHS_RUN"] = epoch
    checkpoint["MODEL_STATE_DICT"] = model.module.state_dict() if distributed else model.state_dict()
    torch.save(checkpoint, ckptpath)
    print(f"checkpoint epoch {epoch} saved.")

def load_checkpoint(model, ckptpath, intest=False):
    checkpoint = torch.load(ckptpath)
    epochs_run = checkpoint["EPOCHS_RUN"]
    model.load_state_dict(checkpoint["MODEL_STATE_DICT"])
    if intest:
        print(f"loading model, epochs run: {epochs_run} form {ckptpath}")
    else:
        print(f"resuming from epoch {epochs_run}...running{epochs_run+1}")
    return model, epochs_run

class Trainer:
    def __init__(self, train_objects: dict, params: dict) -> None:
        self.model = train_objects['model']
        self.optimizer = train_objects['optimizer']
        self.scheduler = train_objects['scheduler']
        self.train_loader = train_objects['train_loader']
        self.val_loader = train_objects['val_loader']
        self.gpu_id = train_objects['gpu_id']
        self.epochs_run = train_objects['epochs_run']
        self.ckptpath = train_objects['ckptpath']
        self.loss_fn = train_objects['loss_fn']
        self.loss_args = train_objects['loss_args']
        self.num_epochs = train_objects['num_epochs']
        self.wandblog =  train_objects['wandblog']
        self.save_every = train_objects['save_every'] 
        self.distributed = train_objects['distributed']
               
        
    def eval_step(self, epoch):
        self.model.eval()
        total = 0.
        loss_sum=0.
        correct=0.
        logits = []
        x_val, y_val=[],[]
        for i, (images, labels) in enumerate(self.val_loader):
            images, labels = images.to(self.gpu_id), labels.to(self.gpu_id)
            outputs = self.model(images)
            loss = self.loss_fn(self.model,
                        images,
                        labels,
                        gpu_id=self.gpu_id,
                        epoch=epoch,
                        **self.loss_args)
            loss_sum += loss.item() 
            predicted = torch.max(outputs, dim=1)[1]
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            logits.append(outputs)
            x_val.append(images)
            y_val.append(labels)
        logits =torch.cat(logits)
        x_val = torch.cat(x_val)
        y_val = torch.cat(y_val)
        eval_acc = 100*correct/total
        log({'val loss':loss_sum/total,
             'val acc': eval_acc,
             'epoch': epoch}, self.wandblog)
        # return x_val, y_val, logits

    def estim_correl_step(self, epoch, x_val, y_val, logits):
        self.model.eval()
        hcorrel, lmcorrel, fab_acc = compute_correlation(self.model,logits,
                                                         x_val, y_test = None,
                                                         device=self.gpu_id,
                                                         norm_thread='Linf')
        log({'ent. correl': hcorrel,
             'lm. correl': lmcorrel,
             'fab acc': fab_acc, 'epoch': epoch}, self.wandblog)

    def train_step(self, epoch):
        self.model.train()
        for i, (images, labels) in enumerate(self.train_loader):  
            images, labels = images.to(self.gpu_id), labels.to(self.gpu_id)
            #Forward pass
            loss = self.loss_fn(self.model,
                        images,
                        labels,
                        gpu_id=self.gpu_id,
                        epoch=epoch,
                        **self.loss_args)

            # Backward and optimize
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            log_condition = (self.distributed and self.gpu_id == 0) or not self.distributed
            if log_condition and (i+1) % 400 == 0:
                lr = self.scheduler.get_last_lr() \
                    if self.scheduler else self.optimizer.param_groups[0]['lr']
                log({'train step': i+1, 
                    'train loss': loss.item(),
                    'epoch': epoch, 'lr':lr}, self.wandblog)

    def train(self):
        #Setting the loss function
        self.model.train()
        for epoch in tqdm(range(self.epochs_run+1, self.num_epochs+1)):
            self.train_step(epoch)
            if self.scheduler:
                self.scheduler.step()
            
            log_condition = (self.distributed and self.gpu_id == 0) or not self.distributed
            if log_condition and epoch % self.save_every  == 0:
                save_checkpoint(self.model, epoch, self.ckptpath, self.distributed)

            if log_condition and self.wandblog:
                self.eval_step(epoch)
                # self.estim_correl_step(epoch)
        #save model
        log_condition = (self.distributed and self.gpu_id == 0) or not self.distributed
        if log_condition and self.epochs_run<self.num_epochs:
            save_checkpoint(self.model, self.num_epochs, self.ckptpath, self.distributed)


def run_experiment(params: dict, args: argparse.Namespace) -> None:
    ddp_setup()
    train_objects = load_train_objects(params, args)
    # torch.autograd.set_detect_anomaly(True)
    # mode = 'online' if params['wandblog'] else 'disabled'
    # wandb.init(project="lvsnAT", name='AT baselines', mode=mode)
    modeltrainer = Trainer(train_objects, params)
    modeltrainer.train()
    # wandb.finish()
    # torch.autograd.set_detect_anomaly(False)
    destroy_process_group()

def main(args: list) -> None:
  """Parse command line args, load training params, and initiate training.

  :param args: command line parameters as list of strings.
  """
  args = parse_args(args)
  paramsfilename = f'./params{args.paramfile}.yaml'
  print(paramsfilename)
  with open(paramsfilename, 'r') as param_file:
      params = yaml.load(param_file, Loader=yaml.SafeLoader)

  if args.loss:
      params['loss'] = args.loss
  if args.dataset:
      params['dataset'] = args.dataset 

  set_seeds(params['seed'])
  run_experiment(params, args)


if __name__ == "__main__":
  main(sys.argv[1:])