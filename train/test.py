# -*- coding: utf-8 -*-
"""cifar10 autoattack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hARkBQpRwNh-iFNBJKLOqKNQEcKk8Qgm
"""

# !pip install git+https://github.com/fra31/auto-attack

# https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/
# Load in relevant libraries, and alias where appropriate
# from autoattack import AutoAttack 
import argparse
import os
import sys
import random
import yaml
import torchmetrics
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
import numpy as np
import numpy as np
import torch
import pandas as pd
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.distributed import init_process_group, destroy_process_group

from autoattack import AutoAttack
from autoattack.state import EvaluationState
# from robustbench.utils import load_model

from pkg.dataset import get_CIFAR10, get_CIFAR100, get_MNIST
from pkg.small_cnn import SmallCNN
from pkg.resnet import ResNet18
from pkg.mutils import ResNetNormed
from distributed_train import load_checkpoint, set_seeds
from pkg.losses import generate_adv


def parse_args(args: list) -> argparse.Namespace:
  """Parse command line parameters.

  :param args: command line parameters as list of strings (for example
      ``["--help"]``).
  :return: command line parameters namespace.
  """
  parser = argparse.ArgumentParser(
      description="Test the models for this experiment."
  )

  parser.add_argument(
      "--no-cuda", action="store_true", default=False, help="disables CUDA training"
  )
  parser.add_argument(
      "--dataset-path",
      default="/home-local2/jongn2.extra.nobkp/data",
      help="the path to the dataset",
      type=str,
  )
  parser.add_argument(
      "--cpus-per-trial",
      default=1,
      help="the number of CPU cores to use per trial",
      type=int,
  )
  parser.add_argument(
      "--project-name",
      help="the name of the Weights and Biases project to save the results",
      # required=True,
      type=str,
  )
  parser.add_argument(
      "--dataset",
      help="dataset used",
      required=False,
      type=str,
  )
  parser.add_argument(
      "--loss",
      help="loss used",
      required=False,
      type=str,
  )
  parser.add_argument(
      "--aattack_version",
      help="attack used",
      required=False,
      type=str,
  )
  parser.add_argument(
      "--paramfile",
      help="param file to use",
      default="",
      required=False,
      type=str,
    )

  return parser.parse_args(args)


def load_test_objects(params, args, distributed=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    gpu_id = int(os.environ["LOCAL_RANK"]) if distributed else device
    prefix = f"{args.project_name}_" if args.project_name else ""
    model_loaded=False
    if params['dataset']=='mnist':
        ckptpath = f"./checkpoints/{prefix}{params['loss']}_ckpt.pt"
        test_dataset = get_MNIST(train=False, pool_size=10000)
        model = SmallCNN()
    else:
        ckptpath = f"./checkpoints/{prefix}{params['loss']}_r18_ckpt.pt"
        test_dataset = get_CIFAR10(train=False)
        if 'loss' in params['loss']:
            model = ResNet18()
        else:
            raise ValueError

    model = model.to(gpu_id)
    if distributed:
        # Make dataloader
        test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                            batch_size = params['batch_size'],
                                            shuffle = False,
                                            sampler = DistributedSampler(test_dataset),
                                            pin_memory=True)
        test_acc =  torchmetrics.Accuracy(num_classes=10, average="micro"
        ).to(gpu_id)
    else:
        test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                                batch_size = params['batch_size'],
                                                shuffle = False,
                                                num_workers=2, pin_memory=True)
    if 'loss' in params['loss'] and os.path.exists(ckptpath):
        model, _ = load_checkpoint(model, ckptpath, intest=True)

    elif not model_loaded:
        emsg = f"{params['loss']} no model loaded"
        raise ValueError(emsg)



    if distributed:
        model = DDP(model, device_ids=[gpu_id])
        return model, gpu_id, test_loader, test_acc

    return model, gpu_id, test_loader


def dist_clean_test(model, test_loader, test_acc, params, gpu_id):
    model.eval()
    with torch.no_grad():
        for images, labels in tqdm(test_loader):
            images, labels = images.to(gpu_id), labels.to(gpu_id)
            outputs = model(images)
            test_acc.update(outputs, labels)
    clean_acc = 100*test_acc.compute().item()
    if gpu_id==0:
        print(f'Accuracy: {clean_acc:.2f} %')
    write_to_csv(params, 'clean acc', clean_acc)

def clean_test(model, test_loader, params, device, forlogits=False):
    model.eval()
    logitslist = []
    labelslist = []
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in tqdm(test_loader):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            logitslist.append(outputs)
            labelslist.append(labels)
    clean_acc = 100 * correct / total
    print(f'GPU:{device} Accuracy on {total} test images: {clean_acc:.2f} %')
    if not forlogits:
        write_to_csv(params, 'clean acc', clean_acc)
    return torch.cat(logitslist).cpu(), torch.cat(labelslist)

def write_to_csv(params, metric, metric_value):
    resfile = f"./results/{params['dataset']}_results.csv"
    metric_value = round(metric_value, 2)
    if os.path.exists(resfile):
        df = pd.read_csv(resfile)
    else:   
        df=pd.DataFrame(columns=["loss"])
    lossname = params['loss'][:-5]
    if len(df[df['loss'] == lossname])>0:
        df.loc[df['loss']==lossname, metric] = metric_value
    else:
        new_row = {'loss':lossname}
        new_row[metric] = metric_value
        df = df.append(new_row, ignore_index=True)
    df.to_csv(resfile, index=False)


def get_XY(test_loader):
    x_test, y_test = [], []
    for i, (x, y) in enumerate(test_loader):
        x_test.append(x)
        y_test.append(y)
    return torch.cat(x_test), torch.cat(y_test)
                     
def eval_auto_attack(model, test_loader,
                    params,
                     device='cpu',
                     version='standard'):
    if version == 'pgd':
        pgd_attack(model, test_loader, params, device)
        return
    model.eval()
    ##
    x_test, y_test = get_XY(test_loader)                       
    x_test, y_test = x_test.to(device), y_test.to(device)
    # print(x_test.shape, y_test.shape)
    attacks_to_run = []
    if version not in ['standard', 'plus', 'rand']:
        attacks_to_run.append(version)
    norm = params[params['dataset']]['adv_test']['norm']
    epsilon = params[params['dataset']]['adv_test']['epsilon']
    adversary = AutoAttack(model,
                            norm=norm,
                            eps=epsilon,
                            version=version,
                            attacks_to_run=attacks_to_run,
                            log_path='./autoattack_dir/autoattack_test.txt')
    lossname = params['loss'][:-5]
    timestamp = str(datetime.now()).split('.')[0][12:]
    evalstate_path = f"./autoattack_dir/{timestamp}_{lossname}_{params['dataset'][0]}_{version}.pt"
    evalstate_path = Path(evalstate_path)
    _ = adversary.run_standard_evaluation(x_test, y_test, state_path=evalstate_path)    
    #
    state = EvaluationState.from_disk(evalstate_path)   
    robacc = 100* state.robust_flags.float().mean().item()
    clean_acc = 100 * state.clean_accuracy
    write_to_csv(params, 'clean acc', clean_acc)
    metric = 'apgd-ce' if version == 'apgd-ce' else 'auto-attack'
    write_to_csv(params, metric, robacc)

def pgd_attack(model,test_loader, params, device='cpu'):
    model.eval()
    ##    
    norm = params[params['dataset']]['adv_test']['norm']
    norm = 2 if norm == 'L2' else float('inf')

    adv_test_params = params[params['dataset']]['adv_test']
    epsilon = adv_test_params['epsilon']
    perturb_steps = adv_test_params['perturb_steps']
    step_size = adv_test_params['step_size']

    adv = 0
    sub_total = 0
    total = 0
    for images, labels in tqdm(test_loader):
        images, labels = images.to(device), labels.to(device)
        x_pgd = generate_adv(model,
                images,
                y=None,
                step_size=step_size,
                epsilon=epsilon,
                perturb_steps=perturb_steps,
                norm='Linf',
                category='madry',
                rand_init=True,
                gairat=False,
                gpu_id=device)
        outputs = model(images)
        adv_outputs = model(x_pgd)
        _, predicted = torch.max(outputs.data, 1)
        _, adv_predicted = torch.max(adv_outputs.data, 1)
        correct_batch = labels.eq(predicted)
        subset_adv = (adv_predicted == predicted)[correct_batch]
        adv += subset_adv.sum().item()
        sub_total += len(subset_adv)
        total += labels.size(0)
    
    pgd_acc = 100 * adv / sub_total
    clean_acc = 100 * sub_total/total
    write_to_csv(params, 'clean acc', clean_acc)
    print(f'pgd: {pgd_acc:.2f}% clean:{clean_acc:.2f}%')
    write_to_csv(params, 'pgd', pgd_acc)

def main(args: list) -> None:
    args = parse_args(args)
    paramsfilename = f'./params.yaml'
    print(paramsfilename)
    with open(paramsfilename, 'r') as param_file:
        params = yaml.load(param_file, Loader=yaml.SafeLoader)
    if args.loss:
        params['loss'] = args.loss
    if args.dataset:
        params['dataset'] = args.dataset
    if args.aattack_version:
        params['aattack_version'] = args.aattack_version 
    # set_seeds(params['seed'])
    set_seeds(0)
    model, device, test_loader  = load_test_objects(params, args)
    eval_auto_attack(model, test_loader, params, device, version=params['aattack_version'])  

if __name__ == "__main__":
  main(sys.argv[1:])
