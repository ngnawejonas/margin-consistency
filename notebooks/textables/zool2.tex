\begin{table}[]
    \begin{tabular}{lllllll}
    \# & Model ID                                 & Paper                                                                                                              & Clean accuracy & Robust accuracy & Architecture     & Venue                                                      \\
    1  & Wang2023Better\_WRN-70-16                & Better Diffusion Models Further Improve Adversarial Training                                                       & 95.54\%        & 84.97\%         & WideResNet-70-16 & arXiv, Feb 2023                                            \\
    2  & Wang2023Better\_WRN-28-10                & Better Diffusion Models Further Improve Adversarial Training                                                       & 95.16\%        & 83.68\%         & WideResNet-28-10 & arXiv, Feb 2023                                            \\
    3  & Rebuffi2021Fixing\_70\_16\_cutmix\_extra & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 95.74\%        & 82.32\%         & WideResNet-70-16 & arXiv, Mar 2021                                            \\
    4  & Gowal2020Uncovering\_extra               & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 94.74\%        & 80.53\%         & WideResNet-70-16 & arXiv, Oct 2020                                            \\
    5  & Rebuffi2021Fixing\_70\_16\_cutmix\_ddpm  & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 92.41\%        & 80.42\%         & WideResNet-70-16 & arXiv, Mar 2021                                            \\
    6  & Rebuffi2021Fixing\_28\_10\_cutmix\_ddpm  & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 91.79\%        & 78.80\%         & WideResNet-28-10 & arXiv, Mar 2021                                            \\
    7  & Augustin2020Adversarial\_34\_10\_extra   & Adversarial Robustness on In- and Out-Distribution Improves Explainability                                         & 93.96\%        & 78.79\%         & WideResNet-34-10 & ECCV 2020                                                  \\
    8  & Sehwag2021Proxy                          & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 90.93\%        & 77.24\%         & WideResNet-34-10 & ICLR 2022                                                  \\
    9  & Augustin2020Adversarial\_34\_10          & Adversarial Robustness on In- and Out-Distribution Improves Explainability                                         & 92.23\%        & 76.25\%         & WideResNet-34-10 & ECCV 2020                                                  \\
    10 & Rade2021Helper\_R18\_ddpm                & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 90.57\%        & 76.15\%         & PreActResNet-18  & OpenReview, Jun 2021                                       \\
    11 & Rebuffi2021Fixing\_R18\_cutmix\_ddpm     & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 90.33\%        & 75.86\%         & PreActResNet-18  & arXiv, Mar 2021                                            \\
    12 & Gowal2020Uncovering                      & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 90.90\%        & 74.50\%         & WideResNet-70-16 & arXiv, Oct 2020                                            \\
    13 & Sehwag2021Proxy\_R18                     & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 89.76\%        & 74.41\%         & ResNet-18        & ICLR 2022                                                  \\
    14 & Wu2020Adversarial                        & Adversarial Weight Perturbation Helps Robust Generalization                                                        & 88.51\%        & 73.66\%         & WideResNet-34-10 & NeurIPS 2020                                               \\
    15 & Augustin2020Adversarial                  & Adversarial Robustness on In- and Out-Distribution Improves Explainability                                         & 91.08\%        & 72.91\%         & ResNet-50        & ECCV 2020                                                  \\
    16 & Engstrom2019Robustness                   & Robustness library                                                                                                 & 90.83\%        & 69.24\%         & ResNet-50        & \begin{tabular}[c]{@{}l@{}}GitHub,\\ Sep 2019\end{tabular} \\
    17 & Rice2020Overfitting                      & Overfitting in adversarially robust deep learning                                                                  & 88.67\%        & 67.68\%         & PreActResNet-18  & ICML 2020                                                  \\
    18 & Rony2019Decoupling                       & Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses                     & 89.05\%        & 66.44\%         & WideResNet-28-10 & CVPR 2019                                                  \\
    19 & Ding2020MMA                              & MMA Training: Direct Input Space Margin Maximization through Adversarial Training                                  & 88.02\%        & 66.09\%         & WideResNet-28-4  & ICLR 2020                                                  \\
    20 & Standard                                 & Standardly trained model                                                                                           & 94.78\%        & 0.00\%          & WideResNet-28-10 & N/A                                                       
    \end{tabular}
    \end{table}