\begin{table}[]
  \begin{tabular}{lllllll}
  \# & Model ID                                      & Paper                                                                                                              & Clean accuracy & Robust accuracy & Architecture                                   & Venue                \\
  1  & Wang2023Better\_WRN-70-16                     & Better Diffusion Models Further Improve Adversarial Training                                                       & 75.22\%        & 42.67\%         & WideResNet-70-16                               & arXiv, Feb 2023      \\
  2  & Cui2023Decoupled\_WRN-28-10                   & Decoupled Kullback-Leibler Divergence Loss                                                                         & 73.85\%        & 39.18\%         & WideResNet-28-10                               & arXiv, May 2023      \\
  3  & Wang2023Better\_WRN-28-10                     & Better Diffusion Models Further Improve Adversarial Training                                                       & 72.58\%        & 38.83\%         & WideResNet-28-10                               & ICML 2023            \\
  4  & Bai2023Improving\_edm                         & Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing                                  & 85.21\%        & 38.72\%         & ResNet-152 + WideResNet-70-16 + mixing network & arXiv, Jan 2023      \\
  5  & Gowal2020Uncovering\_extra                    & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 69.15\%        & 36.88\%         & WideResNet-70-16                               & arXiv, Oct 2020      \\
  6  & Bai2023Improving\_trades                      & Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing                                  & 80.18\%        & 35.15\%         & ResNet-152 + WideResNet-70-16 + mixing network & arXiv, Jan 2023      \\
  7  & Debenedetti2022Light\_XCiT-L12                & A Light Recipe to Train Robust Vision Transformers                                                                 & 70.76\%        & 35.08\%         & XCiT-L12                                       & arXiv, Sep 2022      \\
  8  & Rebuffi2021Fixing\_70\_16\_cutmix\_ddpm       & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 63.56\%        & 34.64\%         & WideResNet-70-16                               & arXiv, Mar 2021      \\
  9  & Debenedetti2022Light\_XCiT-M12                & A Light Recipe to Train Robust Vision Transformers                                                                 & 69.21\%        & 34.21\%         & XCiT-M12                                       & arXiv, Sep 2022      \\
  10 & Pang2022Robustness\_WRN70\_16                 & Robustness and Accuracy Could Be Reconcilable by (Proper) Definition                                               & 65.56\%        & 33.05\%         & WideResNet-70-16                               & ICML 2022            \\
  11 & Cui2023Decoupled\_WRN-34-10\_autoaug          & Decoupled Kullback-Leibler Divergence Loss                                                                         & 65.93\%        & 32.52\%         & WideResNet-34-10                               & arXiv, May 2023      \\
  12 & Debenedetti2022Light\_XCiT-S12                & A Light Recipe to Train Robust Vision Transformers                                                                 & 67.34\%        & 32.19\%         & XCiT-S12                                       & arXiv, Sep 2022      \\
  13 & Rebuffi2021Fixing\_28\_10\_cutmix\_ddpm       & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 62.41\%        & 32.06\%         & WideResNet-28-10                               & arXiv, Mar 2021      \\
  14 & Jia2022LAS-AT\_34\_20                         & LAS-AT: Adversarial Training with Learnable Attack Strategy                                                        & 67.31\%        & 31.91\%         & WideResNet-34-20                               & arXiv, Mar 2022      \\
  15 & Addepalli2022Efficient\_WRN\_34\_10           & Efficient and Effective Augmentation Strategy for Adversarial Training                                             & 68.75\%        & 31.85\%         & WideResNet-34-10                               & NeurIPS 2022         \\
  16 & Cui2023Decoupled\_WRN-34-10                   & Decoupled Kullback-Leibler Divergence Loss                                                                         & 64.08\%        & 31.65\%         & WideResNet-34-10                               & arXiv, May 2023      \\
  17 & Cui2020Learnable\_34\_10\_LBGAT9\_eps\_8\_255 & Learnable Boundary Guided Adversarial Training                                                                     & 62.99\%        & 31.20\%         & WideResNet-34-10                               & ICCV 2021            \\
  18 & Sehwag2021Proxy                               & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 65.93\%        & 31.15\%         & WideResNet-34-10                               & ICLR 2022            \\
  19 & Pang2022Robustness\_WRN28\_10                 & Robustness and Accuracy Could Be Reconcilable by (Proper) Definition                                               & 63.66\%        & 31.08\%         & WideResNet-28-10                               & ICML 2022            \\
  20 & Jia2022LAS-AT\_34\_10                         & LAS-AT: Adversarial Training with Learnable Attack Strategy                                                        & 64.89\%        & 30.77\%         & WideResNet-34-10                               & arXiv, Mar 2022      \\
  21 & Chen2021LTD\_WRN34\_10                        & LTD: Low Temperature Distillation for Robust Adversarial Training                                                  & 64.07\%        & 30.59\%         & WideResNet-34-10                               & arXiv, Nov 2021      \\
  22 & Addepalli2021Towards\_WRN34                   & Scaling Adversarial Training to Large Perturbation Bounds                                                          & 65.73\%        & 30.35\%         & WideResNet-34-10                               & ECCV 2022            \\
  23 & Cui2020Learnable\_34\_20\_LBGAT6              & Learnable Boundary Guided Adversarial Training                                                                     & 62.55\%        & 30.20\%         & WideResNet-34-20                               & ICCV 2021            \\
  24 & Gowal2020Uncovering                           & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 60.86\%        & 30.03\%         & WideResNet-70-16                               & arXiv, Oct 2020      \\
  25 & Cui2020Learnable\_34\_10\_LBGAT6              & Learnable Boundary Guided Adversarial Training                                                                     & 60.64\%        & 29.33\%         & WideResNet-34-10                               & ICCV 2021            \\
  26 & Rade2021Helper\_R18\_ddpm                     & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 61.50\%        & 28.88\%         & PreActResNet-18                                & OpenReview, Jun 2021 \\
  27 & Wu2020Adversarial                             & Adversarial Weight Perturbation Helps Robust Generalization                                                        & 60.38\%        & 28.86\%         & WideResNet-34-10                               & NeurIPS 2020         \\
  28 & Rebuffi2021Fixing\_R18\_ddpm                  & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 56.87\%        & 28.50\%         & PreActResNet-18                                & arXiv, Mar 2021      \\
  29 & Hendrycks2019Using                            & Using Pre-Training Can Improve Model Robustness and Uncertainty                                                    & 59.23\%        & 28.42\%         & WideResNet-28-10                               & ICML 2019            \\
  30 & Addepalli2022Efficient\_RN18                  & Efficient and Effective Augmentation Strategy for Adversarial Training                                             & 65.45\%        & 27.67\%         & ResNet-18                                      & NeurIPS 2022         \\
  31 & Cui2020Learnable\_34\_10\_LBGAT0              & Learnable Boundary Guided Adversarial Training                                                                     & 70.25\%        & 27.16\%         & WideResNet-34-10                               & ICCV 2021            \\
  32 & Addepalli2021Towards\_PARN18                  & Scaling Adversarial Training to Large Perturbation Bounds                                                          & 62.02\%        & 27.14\%         & PreActResNet-18                                & ECCV 2022            \\
  33 & Chen2020Efficient                             & Efficient Robust Training via Backward Smoothing                                                                   & 62.15\%        & 26.94\%         & WideResNet-34-10                               & arXiv, Oct 2020      \\
  34 & Sitawarin2020Improving                        & Improving Adversarial Robustness Through Progressive Hardening                                                     & 62.82\%        & 24.57\%         & WideResNet-34-10                               & arXiv, Mar 2020      \\
  35 & Rice2020Overfitting                           & Overfitting in adversarially robust deep learning                                                                  & 53.83\%        & 18.95\%         & PreActResNet-18                                & ICML 2020           
  \end{tabular}
  \end{table}