\begin{table}[]
    \begin{tabular}{lllllll}
    \# & Model ID                                 & Paper                                                                                                              & Clean accuracy & Robust accuracy & Architecture                                                      & Venue                                                      \\
    1  & Peng2023Robust                           & Robust Principles: Architectural Design Principles for Adversarially Robust CNNs                                   & 93.27\%        & 71.07\%         & RaWideResNet-70-16                                                & BMVC 2023                                                  \\
    2  & Wang2023Better\_WRN-70-16                & Better Diffusion Models Further Improve Adversarial Training                                                       & 93.25\%        & 70.69\%         & WideResNet-70-16                                                  & ICML 2023                                                  \\
    3  & Bai2023Improving\_edm                    & Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing                                  & 95.23\%        & 68.06\%         & ResNet-152 + WideResNet-70-16 + mixing network                    & arXiv, Jan 2023                                            \\
    4  & Cui2023Decoupled\_WRN-28-10              & Decoupled Kullback-Leibler Divergence Loss                                                                         & 92.16\%        & 67.73\%         & WideResNet-28-10                                                  & arXiv, May 2023                                            \\
    5  & Wang2023Better\_WRN-28-10                & Better Diffusion Models Further Improve Adversarial Training                                                       & 92.44\%        & 67.31\%         & WideResNet-28-10                                                  & ICML 2023                                                  \\
    6  & Rebuffi2021Fixing\_70\_16\_cutmix\_extra & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 92.23\%        & 66.56\%         & WideResNet-70-16                                                  & arXiv, Mar 2021                                            \\
    7  & Gowal2021Improving\_70\_16\_ddpm\_100m   & Improving Robustness using Generated Data                                                                          & 88.74\%        & 66.10\%         & WideResNet-70-16                                                  & NeurIPS 2021                                               \\
    8  & Gowal2020Uncovering\_70\_16\_extra       & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 91.10\%        & 65.87\%         & WideResNet-70-16                                                  & arXiv, Oct 2020                                            \\
    9  & Huang2022Revisiting\_WRN-A4              & Revisiting Residual Networks for Adversarial Robustness: An Architectural Perspective                              & 91.58\%        & 65.79\%         & WideResNet-A4                                                     & arXiv, Dec. 2022                                           \\
    10 & Rebuffi2021Fixing\_106\_16\_cutmix\_ddpm & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 88.50\%        & 64.58\%         & WideResNet-106-16                                                 & arXiv, Mar 2021                                            \\
    11 & Rebuffi2021Fixing\_70\_16\_cutmix\_ddpm  & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 88.54\%        & 64.20\%         & WideResNet-70-16                                                  & arXiv, Mar 2021                                            \\
    12 & Kang2021Stable                           & Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks                & 93.73\%        & 64.20\%         & WideResNet-70-16, Neural ODE block                                & NeurIPS 2021                                               \\
    13 & Xu2023Exploring\_WRN-28-10               & Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness                                     & 93.69\%        & 63.89\%         & WideResNet-28-10                                                  & ICLR 2023                                                  \\
    14 & Gowal2021Improving\_28\_10\_ddpm\_100m   & Improving Robustness using Generated Data                                                                          & 87.50\%        & 63.38\%         & WideResNet-28-10                                                  & NeurIPS 2021                                               \\
    15 & Pang2022Robustness\_WRN70\_16            & Robustness and Accuracy Could Be Reconcilable by (Proper) Definition                                               & 89.01\%        & 63.35\%         & WideResNet-70-16                                                  & ICML 2022                                                  \\
    16 & Rade2021Helper\_extra                    & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 91.47\%        & 62.83\%         & WideResNet-34-10                                                  & OpenReview, Jun 2021                                       \\
    17 & Sehwag2021Proxy\_ResNest152              & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 87.30\%        & 62.79\%         & ResNest152                                                        & ICLR 2022                                                  \\
    18 & Gowal2020Uncovering\_28\_10\_extra       & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 89.48\%        & 62.76\%         & WideResNet-28-10                                                  & arXiv, Oct 2020                                            \\
    19 & Huang2021Exploring\_ema                  & Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks                                   & 91.23\%        & 62.54\%         & WideResNet-34-R                                                   & NeurIPS 2021                                               \\
    20 & Huang2021Exploring                       & Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks                                   & 90.56\%        & 61.56\%         & WideResNet-34-R                                                   & NeurIPS 2021                                               \\
    21 & Dai2021Parameterizing                    & Parameterizing Activation Functions for Adversarial Robustness                                                     & 87.02\%        & 61.55\%         & WideResNet-28-10-PSSiLU                                           & arXiv, Oct 2021                                            \\
    22 & Pang2022Robustness\_WRN28\_10            & Robustness and Accuracy Could Be Reconcilable by (Proper) Definition                                               & 88.61\%        & 61.04\%         & WideResNet-28-10                                                  & ICML 2022                                                  \\
    23 & Rade2021Helper\_ddpm                     & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 88.16\%        & 60.97\%         & WideResNet-28-10                                                  & OpenReview, Jun 2021                                       \\
    24 & Rebuffi2021Fixing\_28\_10\_cutmix\_ddpm  & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 87.33\%        & 60.73\%         & WideResNet-28-10                                                  & arXiv, Mar 2021                                            \\
    25 & Sridhar2021Robust\_34\_15                & Improving Neural Network Robustness via Persistency of Excitation                                                  & 86.53\%        & 60.41\%         & WideResNet-34-15                                                  & ACC 2022                                                   \\
    26 & Sehwag2021Proxy                          & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 86.68\%        & 60.27\%         & WideResNet-34-10                                                  & ICLR 2022                                                  \\
    27 & Wu2020Adversarial\_extra                 & Adversarial Weight Perturbation Helps Robust Generalization                                                        & 88.25\%        & 60.04\%         & WideResNet-28-10                                                  & NeurIPS 2020                                               \\
    28 & Sridhar2021Robust                        & Improving Neural Network Robustness via Persistency of Excitation                                                  & 89.46\%        & 59.66\%         & WideResNet-28-10                                                  & ACC 2022                                                   \\
    29 & Zhang2020Geometry                        & Geometry-aware Instance-reweighted Adversarial Training                                                            & 89.36\%        & 59.64\%         & WideResNet-28-10                                                  & ICLR 2021                                                  \\
    30 & Carmon2019Unlabeled                      & Unlabeled Data Improves Adversarial Robustness                                                                     & 89.69\%        & 59.53\%         & WideResNet-28-10                                                  & NeurIPS 2019                                               \\
    31 & Gowal2021Improving\_R18\_ddpm\_100m      & Improving Robustness using Generated Data                                                                          & 87.35\%        & 58.50\%         & PreActResNet-18                                                   & NeurIPS 2021                                               \\
    32 & Addepalli2021Towards\_WRN34              & Scaling Adversarial Training to Large Perturbation Bounds                                                          & 85.32\%        & 58.04\%         & WideResNet-34-10                                                  & ECCV 2022                                                  \\
    33 & Addepalli2022Efficient\_WRN\_34\_10      & Efficient and Effective Augmentation Strategy for Adversarial Training                                             & 88.71\%        & 57.81\%         & WideResNet-34-10                                                  & NeurIPS 2022                                               \\
    34 & Chen2021LTD\_WRN34\_20                   & LTD: Low Temperature Distillation for Robust Adversarial Training                                                  & 86.03\%        & 57.71\%         & WideResNet-34-20                                                  & arXiv, Nov 2021                                            \\
    35 & Rade2021Helper\_R18\_extra               & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 89.02\%        & 57.67\%         & PreActResNet-18                                                   & OpenReview, Jun 2021                                       \\
    36 & Jia2022LAS-AT\_70\_16                    & LAS-AT: Adversarial Training with Learnable Attack Strategy                                                        & 85.66\%        & 57.61\%         & WideResNet-70-16                                                  & arXiv, Mar 2022                                            \\
    37 & Debenedetti2022Light\_XCiT-L12           & A Light Recipe to Train Robust Vision Transformers                                                                 & 91.73\%        & 57.58\%         & XCiT-L12                                                          & arXiv, Sep 2022                                            \\
    38 & Debenedetti2022Light\_XCiT-M12           & A Light Recipe to Train Robust Vision Transformers                                                                 & 91.30\%        & 57.27\%         & XCiT-M12                                                          & arXiv, Sep 2022                                            \\
    39 & Sehwag2020Hydra                          & HYDRA: Pruning Adversarially Robust Neural Networks                                                                & 88.98\%        & 57.14\%         & WideResNet-28-10                                                  & NeurIPS 2020                                               \\
    40 & Gowal2020Uncovering\_70\_16              & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 85.29\%        & 57.14\%         & WideResNet-70-16                                                  & arXiv, Oct 2020                                            \\
    41 & Rade2021Helper\_R18\_ddpm                & Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off & 86.86\%        & 57.09\%         & PreActResNet-18                                                   & OpenReview, Jun 2021                                       \\
    42 & Cui2023Decoupled\_WRN-34-10              & Decoupled Kullback-Leibler Divergence Loss                                                                         & 85.31\%        & 57.09\%         & WideResNet-34-10                                                  & arXiv, May 2023                                            \\
    43 & Chen2021LTD\_WRN34\_10                   & LTD: Low Temperature Distillation for Robust Adversarial Training                                                  & 85.21\%        & 56.94\%         & WideResNet-34-10                                                  & arXiv, Nov 2021                                            \\
    44 & Gowal2020Uncovering\_34\_20              & Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples                            & 85.64\%        & 56.82\%         & WideResNet-34-20                                                  & arXiv, Oct 2020                                            \\
    45 & Rebuffi2021Fixing\_R18\_ddpm             & Fixing Data Augmentation to Improve Adversarial Robustness                                                         & 83.53\%        & 56.66\%         & PreActResNet-18                                                   & arXiv, Mar 2021                                            \\
    46 & Wang2020Improving                        & Improving Adversarial Robustness Requires Revisiting Misclassified Examples                                        & 87.50\%        & 56.29\%         & WideResNet-28-10                                                  & ICLR 2020                                                  \\
    47 & Jia2022LAS-AT\_34\_10                    & LAS-AT: Adversarial Training with Learnable Attack Strategy                                                        & 84.98\%        & 56.26\%         & WideResNet-34-10                                                  & arXiv, Mar 2022                                            \\
    48 & Wu2020Adversarial                        & Adversarial Weight Perturbation Helps Robust Generalization                                                        & 85.36\%        & 56.17\%         & WideResNet-34-10                                                  & NeurIPS 2020                                               \\
    49 & Debenedetti2022Light\_XCiT-S12           & A Light Recipe to Train Robust Vision Transformers                                                                 & 90.06\%        & 56.14\%         & XCiT-S12                                                          & arXiv, Sep 2022                                            \\
    50 & Sehwag2021Proxy\_R18                     & Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?                   & 84.59\%        & 55.54\%         & ResNet-18                                                         & ICLR 2022                                                  \\
    51 & Hendrycks2019Using                       & Using Pre-Training Can Improve Model Robustness and Uncertainty                                                    & 87.11\%        & 54.92\%         & WideResNet-28-10                                                  & ICML 2019                                                  \\
    52 & Pang2020Boosting                         & Boosting Adversarial Training with Hypersphere Embedding                                                           & 85.14\%        & 53.74\%         & WideResNet-34-20                                                  & NeurIPS 2020                                               \\
    53 & Cui2020Learnable\_34\_20                 & Learnable Boundary Guided Adversarial Training                                                                     & 88.70\%        & 53.57\%         & WideResNet-34-20                                                  & ICCV 2021                                                  \\
    54 & Zhang2020Attacks                         & Attacks Which Do Not Kill Training Make Adversarial Learning Stronger                                              & 84.52\%        & 53.51\%         & WideResNet-34-10                                                  & ICML 2020                                                  \\
    55 & Rice2020Overfitting                      & Overfitting in adversarially robust deep learning                                                                  & 85.34\%        & 53.42\%         & WideResNet-34-20                                                  & ICML 2020                                                  \\
    56 & Huang2020Self                            & Self-Adaptive Training: beyond Empirical Risk Minimization                                                         & 83.48\%        & 53.34\%         & WideResNet-34-10                                                  & NeurIPS 2020                                               \\
    57 & Zhang2019Theoretically                   & Theoretically Principled Trade-off between Robustness and Accuracy                                                 & 84.92\%        & 53.08\%         & WideResNet-34-10                                                  & ICML 2019                                                  \\
    58 & Cui2020Learnable\_34\_10                 & Learnable Boundary Guided Adversarial Training                                                                     & 88.22\%        & 52.86\%         & WideResNet-34-10                                                  & ICCV 2021                                                  \\
    59 & Addepalli2022Efficient\_RN18             & Efficient and Effective Augmentation Strategy for Adversarial Training                                             & 85.71\%        & 52.48\%         & ResNet-18                                                         & NeurIPS 2022                                               \\
    60 & Chen2020Adversarial                      & Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning                                           & 86.04\%        & 51.56\%         & \begin{tabular}[c]{@{}l@{}}ResNet-50\\ (3x ensemble)\end{tabular} & CVPR 2020                                                  \\
    61 & Chen2020Efficient                        & Efficient Robust Training via Backward Smoothing                                                                   & 85.32\%        & 51.12\%         & WideResNet-34-10                                                  & arXiv, Oct 2020                                            \\
    62 & Addepalli2021Towards\_RN18               & Scaling Adversarial Training to Large Perturbation Bounds                                                          & 80.24\%        & 51.06\%         & ResNet-18                                                         & ECCV 2022                                                  \\
    63 & Sitawarin2020Improving                   & Improving Adversarial Robustness Through Progressive Hardening                                                     & 86.84\%        & 50.72\%         & WideResNet-34-10                                                  & arXiv, Mar 2020                                            \\
    64 & Engstrom2019Robustness                   & Robustness library                                                                                                 & 87.03\%        & 49.25\%         & ResNet-50                                                         & \begin{tabular}[c]{@{}l@{}}GitHub,\\ Oct 2019\end{tabular} \\
    65 & Zhang2019You                             & You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle                                   & 87.20\%        & 44.83\%         & WideResNet-34-10                                                  & NeurIPS 2019                                               \\
    66 & Andriushchenko2020Understanding          & Understanding and Improving Fast Adversarial Training                                                              & 79.84\%        & 43.93\%         & PreActResNet-18                                                   & NeurIPS 2020                                               \\
    67 & Wong2020Fast                             & Fast is better than free: Revisiting adversarial training                                                          & 83.34\%        & 43.21\%         & PreActResNet-18                                                   & ICLR 2020                                                  \\
    68 & Ding2020MMA                              & MMA Training: Direct Input Space Margin Maximization through Adversarial Training                                  & 84.36\%        & 41.44\%         & WideResNet-28-4                                                   & ICLR 2020                                                  \\
    69 & Standard                                 & Standardly trained model                                                                                           & 94.78\%        & 0.00\%          & WideResNet-28-10                                                  & N/A                                                       
    \end{tabular}
    \end{table}